{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers \"sagemaker\" \"s3fs\" \"boto3\" \"pandas\" \"gremlinpython\" \"numpy\" \"seaborn\" \"matplotlib\" \"scikit-learn\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loyalty Abuse Preliminary Code\n",
    "# Import libraries\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import sagemaker\n",
    "from sagemaker import RandomCutForest\n",
    "import boto3\n",
    "import asyncio\n",
    "import gremlin_python\n",
    "from gremlin_python.structure.graph import Graph\n",
    "from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection\n",
    "from gremlin_python.process.graph_traversal import __\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #use this code if you are running on SageMaker Studio\n",
    "    #role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    #use this code if you are running locally\n",
    "    #TODO delete ARN when pushing to github\n",
    "    role_name='your-role-arn'\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client('sagemaker', region_name=sagemaker_session.boto_region_name)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:  Generate Complex Synthetic Data\n",
    "### Helper functions\n",
    "This code cell generates complex synthetic user data for a loyalty program trial, including both normal and abusive user accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Generate Complex Synthetic Data\n",
    "def generate_complex_synthetic_user_data(num_users=1000, abuse_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Generates complex synthetic data for loyalty program trial abuse with additional features.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    abuse_count = int(num_users * abuse_percentage)\n",
    "    normal_count = num_users - abuse_count\n",
    "   \n",
    "    email_domains = ['gmail.com', 'yahoo.com', 'hotmail.com']\n",
    "    locations = ['New York', 'California', 'Texas', 'London', 'Berlin']\n",
    "   \n",
    "    for i in range(normal_count):\n",
    "        data.append({\n",
    "            'user_id': f\"user_{i}\",\n",
    "            'email': f\"user{i}@{random.choice(email_domains)}\",\n",
    "            'ip_address': f\"{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}\",\n",
    "            'device_id': f\"device_{random.randint(1000, 9999)}\",\n",
    "            'location': random.choice(locations),\n",
    "            'trial_uses': random.randint(0, 2),\n",
    "            'transaction_value': round(random.uniform(50, 500), 2),\n",
    "            'transaction_count': random.randint(1, 10),\n",
    "            'time_spent': round(random.uniform(5, 100), 2),  # Time spent in loyalty program (minutes)\n",
    "            'loyalty_points': random.randint(100, 10000),\n",
    "            'signup_date': datetime.now() - timedelta(days=random.randint(30, 365)),\n",
    "            'abusive_account':0\n",
    "        })\n",
    "   \n",
    "    # Add abusive users\n",
    "    for i in range(abuse_count):\n",
    "        data.append({\n",
    "            'user_id': f\"abuser_{i}\",\n",
    "            'email': f\"abuser{i}@{random.choice(email_domains)}\",\n",
    "            'ip_address': f\"{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}\",\n",
    "            'device_id': f\"device_{random.randint(1000, 9999)}\",\n",
    "            'location': random.choice(locations),\n",
    "            'trial_uses': random.randint(3, 10),  # Abusive users exploit multiple trials\n",
    "            'transaction_value': round(random.uniform(10, 50), 2),  # Lower transaction values\n",
    "            'transaction_count': random.randint(5, 50),  # Abusers perform many small transactions\n",
    "            'time_spent': round(random.uniform(1, 10), 2),  # Less time spent\n",
    "            'loyalty_points': random.randint(10, 100),  # Abusive users have few points\n",
    "            'signup_date': datetime.now() - timedelta(days=random.randint(1, 30))  # Recent signups\n",
    "            ,'abusive_account':1  \n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "df = generate_complex_synthetic_user_data(num_users=10000,abuse_percentage=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code extracts the 'abusive_account' column from the dataframe and stores it in a separate variable, then drops the same column from the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abusive_accounts=df['abusive_account']\n",
    "#Drop column from dataframe\n",
    "df.drop(['abusive_account'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs exploratory data analysis (EDA) on the dataset, which is a crucial step in understanding the data and identifying potential issues or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "excluded_columns = {'user_id', 'email', 'ip_address', 'device_id', 'location'}\n",
    "columns = [column for column in df.columns if column not in excluded_columns]\n",
    "\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics for the dataset:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Plot distribution of trial uses and transaction values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['trial_uses'], bins=20, kde=True)\n",
    "plt.title('Distribution of Trial Uses')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['transaction_value'], bins=20, kde=True)\n",
    "plt.title('Distribution of Transaction Value')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[columns].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Time spent vs Transaction Value (for detecting anomalies)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='time_spent', y='transaction_value', hue='trial_uses', data=df)\n",
    "plt.title('Time Spent vs Transaction Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code preprocesses the data by scaling numerical features and saving the preprocessed data to a CSV file in the 'dataset' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Preprocess Data\n",
    "# Preprocess for the model\n",
    "\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_scaled = scaler.fit_transform(df[['trial_uses', 'transaction_value', 'transaction_count', 'time_spent', 'loyalty_points']])\n",
    "np.savetxt('dataset/train.csv', df_scaled, fmt='%s',delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uploads the preprocessed training data to an Amazon S3 bucket for use in model training with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload preprocessed data to S3 for training\n",
    "train_file = 'loyalty-program-abuse/train.csv'\n",
    "train_path = f's3://{sagemaker_session.default_bucket()}/{train_file}'\n",
    "pd.DataFrame(df_scaled).to_csv(train_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train an Anomaly Detection Model (RCF & Autoencoder)\n",
    "### Training with Random Cut Forest (RCF) built-in algorithm in Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up an Amazon SageMaker Random Cut Forest (RCF) model for anomaly detection, specifying the execution role, instance configuration, and output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Train an Anomaly Detection Model (RCF)\n",
    "# Setup RCF model using Amazon SageMaker\n",
    "rcf = RandomCutForest(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{sagemaker_session.default_bucket()}/loyalty-program-abuse/output/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell trains a Random Cut Forest (RCF) model using the scaled data from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RCF model\n",
    "rcf_train_data = rcf.record_set(df_scaled)\n",
    "rcf.fit(rcf_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an custom autoencoder algorithm with Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up an autoencoder model using Amazon SageMaker's PyTorch estimator, specifying various configurations such as the entry point, source directory, AWS role, framework version, instance type, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup autoencoder model using Amazon SageMaker\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "autoencoder = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='scripts',\n",
    "    role=role,\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{sagemaker_session.default_bucket()}/loyalty-program-abuse/output/',\n",
    "    hyperparameters={\n",
    "        'epochs': 100,\n",
    "        'batch-size': 32,\n",
    "        'learning-rate': 0.001,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code trains the autoencoder model using the training data stored in an Amazon S3 bucket location specified by the SageMaker session and train_file variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit({'training': f's3://{sagemaker_session.default_bucket()}/{train_file}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [**TODO**] this section is not ready yet \n",
    "# Step 5: Deploy Models for a real-time detection endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have already trained our PyTorch and RCF models\n",
    "pytorch_model_data = autoencoder.model_data\n",
    "rcf_model_data = rcf.model_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(\n",
    "    model_data=pytorch_model_data,  # Path to your local model artifacts\n",
    "    role=role,  # This is ignored in local mode\n",
    "    entry_point='inference.py',\n",
    "    source_dir='scripts',\n",
    "    framework_version='1.8.0',  # Use the version that matches your training\n",
    "    py_version='py3',\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model containers\n",
    "pytorch_container = pytorch_model.prepare_container_def(instance_type='ml.m5.xlarge')\n",
    "rcf_container = rcf.create_model().prepare_container_def(instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multi-model\n",
    "multi_model_name = 'multi-model-pytorch-rcf'\n",
    "create_model_response = sagemaker_session.boto_session.client('sagemaker').create_model(\n",
    "    ModelName=multi_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[pytorch_container,rcf_container]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create endpoint configuration\n",
    "endpoint_config_name = 'multi-model-endpoint-config'\n",
    "create_endpoint_config_response = sagemaker_session.boto_session.client('sagemaker').create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': multi_model_name,\n",
    "        'VariantName': 'AllTraffic'\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create endpoint\n",
    "endpoint_name = 'multi-model-pytorch-rcf-endpoint'\n",
    "create_endpoint_response = sagemaker_session.boto_session.client('sagemaker').create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f\"Endpoint '{endpoint_name}' is being created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import NumpySerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer\n",
    "\n",
    "# Set up the SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Define your endpoint name\n",
    "endpoint_name = 'multi-model-pytorch-rcf-endpoint'\n",
    "\n",
    "# Create a Predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=NumpySerializer(),\n",
    "    deserializer=NumpyDeserializer()\n",
    ")\n",
    "\n",
    "# Function to perform inference\n",
    "def perform_inference(data):\n",
    "    # Ensure the data is 2-dimensional\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(1, -1)\n",
    "    \n",
    "    # Perform prediction\n",
    "    result = predictor.predict(data)\n",
    "    return result\n",
    "\n",
    "# Example: Inference\n",
    "data = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float32)\n",
    "result = perform_inference(df_scaled)\n",
    "print(\"Inference Result:\", result)\n",
    "\n",
    "# If you're using an RCF model, you might want to use a 2D array instead:\n",
    "# data = np.array([[1.0, 2.0, 3.0, 4.0, 5.0]], dtype=np.float32)\n",
    "# result = perform_inference(data)\n",
    "# print(\"Inference Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Deploy the Model for real-time detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code deploys the trained Random Forest Classifier (RFC) model to a SageMaker endpoint for real-time fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Deploy the Model for real-time detection\n",
    "import time\n",
    "endpoint_name=f'loyalty-abuse-prediction-rfc-{int(time.time())}'\n",
    "predictor_1 = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up an Amazon SageMaker PyTorchModel for deployment and creates an endpoint for making inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=autoencoder.model_data,  # Path to your local model artifacts\n",
    "    role=role,  # This is ignored in local mode\n",
    "    entry_point='inference.py',\n",
    "    source_dir='scripts',\n",
    "    framework_version='1.8.0',  # Use the version that matches your training\n",
    "    py_version='py3',\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "endpoint_name=f'loyalty-abuse-prediction-autoencoder-{int(time.time())}'\n",
    "predictor_2 = model.deploy(\n",
    "                             initial_instance_count=1,\n",
    "                             endpoint_name=endpoint_name,\n",
    "                             instance_type='ml.m4.xlarge')\n",
    "\n",
    "#Increase the timeout to 120\n",
    "config = Config(read_timeout=120, connect_timeout=120)\n",
    "predictor_2.sagemaker_session.sagemaker_runtime_client = boto3.client('sagemaker-runtime', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Detect Anomalies\n",
    "The code block creates two copies of the original dataframe, computes anomaly scores using a pre-trained predictor, adds the anomaly scores as a new column to one of the dataframes, and flags the top 5% anomalies as suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Detect Anomalies\n",
    "# Use the deployed model to predict anomalies in the dataset\n",
    "df_rfc=df\n",
    "df_autoencoder=df\n",
    "anomaly_scores = predictor_1.predict(df_scaled)\n",
    "anomalies = np.array([record.label['score'].float32_tensor.values[0] for record in anomaly_scores])\n",
    "\n",
    "# Add anomaly scores to the original dataframe\n",
    "df_rfc['anomaly_score'] = anomalies\n",
    "\n",
    "# Flag top 5% anomalies\n",
    "df_rfc['is_suspicious'] = df_rfc['anomaly_score'] > np.percentile(df_rfc['anomaly_score'], 95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell calculates anomaly scores, sets a threshold based on the mean and standard deviation of the scores, and flags the top 5% instances as suspicious anomalies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores = predictor_2.predict(df_scaled)\n",
    "# You can then set a threshold to determine which instances are anomalies\n",
    "threshold = np.mean(anomaly_scores) + 2 * np.std(anomaly_scores)\n",
    "anomalies = anomaly_scores > threshold\n",
    "\n",
    "df_autoencoder['anomaly_score'] = anomaly_scores.flatten()\n",
    "\n",
    "# Ensure anomalies is a 1D array\n",
    "anomalies = anomalies.flatten()\n",
    "\n",
    "# Flag top 5% anomalies\n",
    "df_autoencoder['is_suspicious'] = anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 8: Statistical Analysis\n",
    "This code performs statistical analysis on the anomaly scores generated by the fraud detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 8: Statistical Analysis\n",
    "print(\"\\nSummary statistics for the anomaly scores(RFC algorithm):\")\n",
    "print(df_rfc['anomaly_score'].describe())\n",
    "\n",
    "# Perform statistical test to assess anomaly score distribution (e.g., normality)\n",
    "stat, p_value = stats.shapiro(df_rfc['anomaly_score'])\n",
    "print(f\"\\nShapiro-Wilk test for normality: stat={stat}, p-value={p_value}\")\n",
    "if p_value > 0.05:\n",
    "    print(\"Anomaly scores are normally distributed.\")\n",
    "else:\n",
    "    print(\"Anomaly scores are not normally distributed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary statistics for the anomaly scores(Autoencoder):\")\n",
    "print(df_autoencoder['anomaly_score'].describe())\n",
    "\n",
    "# Perform statistical test to assess anomaly score distribution (e.g., normality)\n",
    "stat, p_value = stats.shapiro(df_autoencoder['anomaly_score'])\n",
    "print(f\"\\nShapiro-Wilk test for normality: stat={stat}, p-value={p_value}\")\n",
    "if p_value > 0.05:\n",
    "    print(\"Anomaly scores are normally distributed.\")\n",
    "else:\n",
    "    print(\"Anomaly scores are not normally distributed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 9: Visualize Anomalies\n",
    "The next cells code visualizes the distribution of anomaly scores and highlights the suspicious users based on their anomaly scores and user IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Visualize Anomalies\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_rfc['anomaly_score'], bins=50, kde=True)\n",
    "plt.title(\"Anomaly Score Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Plot suspicious users\n",
    "suspicious_df = df_rfc[df_rfc['is_suspicious']]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='user_id', y='anomaly_score', data=suspicious_df)\n",
    "plt.title(\"Suspicious Users Based on Anomaly Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_autoencoder['anomaly_score'], bins=50, kde=True)\n",
    "plt.title(\"Anomaly Score Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Plot suspicious users\n",
    "suspicious_df = df_autoencoder[df_autoencoder['is_suspicious']]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='user_id', y='anomaly_score', data=suspicious_df)\n",
    "plt.title(\"Suspicious Users Based on Anomaly Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Delete Sagemaker Endpoints\n",
    "This code cell defines a function called delete_sagemaker_endpoints() that uses the boto3 library to list and delete any existing SageMaker endpoints with names starting with 'loyalty-abuse-prediction'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Delete Sagemaker Endpoints\n",
    "import boto3\n",
    "def delete_sagemaker_endpoints():\n",
    "    sm = boto3.client('sagemaker')\n",
    "    endpoints = sm.list_endpoints()['Endpoints']\n",
    "    for endpoint in endpoints:\n",
    "        sagemaker_endpoint_name = endpoint['EndpointName']\n",
    "        if sagemaker_endpoint_name.startswith('loyalty-abuse-prediction'):\n",
    "            print(f'Deleting endpoint: {sagemaker_endpoint_name}')\n",
    "            sm.delete_endpoint(EndpointName=sagemaker_endpoint_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet uses the AWS SDK for Python (Boto3) to list all the existing SageMaker endpoints in your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "endpoints = sm.list_endpoints()['Endpoints']\n",
    "endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the active endpoints that start with 'loyalty-abuse-prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_sagemaker_endpoints()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
